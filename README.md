# DualLLM TruthBot

A production-ready full-stack multi-LLM chatbot that queries Gemini 1.5 Flash + GPT-4o-mini simultaneously, compares responses using a judge LLM, and shows users the "most accurate" answer with reasoning.

## ğŸš€ Features

- âœ… **Dual LLM Comparison**: Gemini 1.5 Flash vs GPT-4o-mini
- âœ… **AI Judge**: GPT-4o-mini evaluates which response is better
- âœ… **Split-screen UI**: Best answer + comparison view
- âœ… **Confidence Scores**: 0-100% confidence ratings
- âœ… **Chat History**: SQLite database storage
- âœ… **Real-time Responses**: Parallel API calls for speed
- âœ… **Copy/Share**: Easy response sharing
- âœ… **Error Handling**: Graceful API failure handling
- âœ… **Docker Ready**: Full containerization

## ğŸ›  Tech Stack

**Backend**: FastAPI + LiteLLM + Uvicorn + Python 3.11  
**Frontend**: React 18 + Vite + TypeScript + Tailwind CSS  
**Database**: SQLite (chat history)  
**Deployment**: Docker Compose ready

## ğŸ“¦ Quick Start

### 1. Clone and Setup Environment

```bash
git clone <repo-url>
cd ChatBot

# Copy environment file
cp backend/.env.example backend/.env
```

### 2. Add API Keys

Edit `backend/.env`:
```env
OPENAI_API_KEY=your_openai_key_here
GEMINI_API_KEY=your_gemini_key_here
DATABASE_URL=sqlite:///./truthbot.db
ORIGINS=http://localhost:3000
```

### 3. Run with Docker Compose

```bash
docker compose up --build
```

Visit: http://localhost:3000

### 4. Local Development

**Backend:**
```bash
cd backend
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8000
```

**Frontend:**
```bash
cd frontend
npm install
npm run dev
```

## ğŸ¯ How It Works

1. **User Query** â†’ Frontend sends to `/api/chat`
2. **Parallel LLM Calls** â†’ FastAPI queries both Gemini & OpenAI simultaneously
3. **Judge Evaluation** â†’ GPT-4o-mini compares responses and picks winner
4. **Response Display** â†’ UI shows best answer + comparison + reasoning
5. **Database Storage** â†’ Chat history saved with timestamps

## ğŸ“ Project Structure

```
ChatBot/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â”‚   â”œâ”€â”€ models.py            # SQLAlchemy models
â”‚   â”‚   â”œâ”€â”€ schemas.py           # Pydantic schemas
â”‚   â”‚   â”œâ”€â”€ database.py          # DB connection
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ llm_service.py   # LLM logic + judge
â”‚   â”‚   â””â”€â”€ routers/
â”‚   â”‚       â””â”€â”€ chat.py          # Chat endpoints
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ .env
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatWindow.tsx   # Main chat interface
â”‚   â”‚   â”‚   â”œâ”€â”€ MessageBubble.tsx # Message display
â”‚   â”‚   â”‚   â””â”€â”€ ChatInput.tsx    # Input component
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â””â”€â”€ useChat.ts       # Chat logic hook
â”‚   â”‚   â””â”€â”€ lib/
â”‚   â”‚       â””â”€â”€ utils.ts         # API utilities
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â””â”€â”€ docker-compose.yml
```

## ğŸ¨ UI Preview

```
â”Œâ”€ DualLLM TruthBot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Query: "Best Python framework 2025?"           â”‚
â”œâ”€ ğŸ† BEST (95% conf) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ FastAPI - Judge: "Most complete reasoning"     â”‚
â”œâ”€ COMPARISON â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ¦’ Gemini: "Django..." | ğŸ”¥ OpenAI: "FastAPI" â”‚
â”‚ âš–ï¸ Judge: "OpenAI more current + benchmarks"   â”‚
â””â”€ [Type your message...] [Send] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ API Endpoints

- `POST /api/chat` - Send chat message
- `GET /api/history/{session_id}` - Get chat history
- `GET /` - API status
- `GET /health` - Health check
- `GET /docs` - FastAPI documentation

## ğŸš€ Production Deployment

### Vercel (Frontend)
```bash
cd frontend
npm run build
# Deploy dist/ folder to Vercel
```

### Render (Backend)
```bash
# Push to GitHub
# Connect Render to repo
# Set environment variables in Render dashboard
```

### Environment Variables for Production
```env
OPENAI_API_KEY=sk-proj-...
GEMINI_API_KEY=AIzaSy...
DATABASE_URL=sqlite:///./truthbot.db
ORIGINS=https://your-frontend-domain.com
```

## ğŸ” Judge Logic

The judge uses this prompt structure:
```
Query: {user_question}
Response A (Gemini): {gemini_response}
Response B (OpenAI): {openai_response}

Pick BEST (A/B/tie). Explain why in 1 sentence.
Return JSON: {"winner": "A", "reason": "explanation", "confidence": 0.95}
```

## ğŸ›¡ Error Handling

- API key validation
- Rate limiting (10 req/min)
- Graceful LLM API failures
- Database connection errors
- Network timeout handling

## ğŸ“Š Performance

- **Parallel API calls** reduce response time by ~50%
- **SQLite** for fast local storage
- **React 18** with optimized re-renders
- **Vite** for fast development builds

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch
3. Add tests for new features
4. Submit pull request

## ğŸ“„ License

MIT License - see LICENSE file for details

---

**Built with â¤ï¸ using FastAPI + React + LiteLLM**